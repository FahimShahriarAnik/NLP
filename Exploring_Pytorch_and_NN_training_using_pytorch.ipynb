{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p2KBzAtlZEdD",
        "V1W_Pq1EYgib",
        "hBgX5a9TZfy3",
        "A4ocJ5ZVaPYM",
        "g-4hUErKaZvG",
        "-GPeNJata0I2",
        "YbC5kdSUc1WM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FahimShahriarAnik/NLP/blob/main/Exploring_Pytorch_and_NN_training_using_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this lab, you will be constructing a neural network in pytorch. Below, there is an introduction to pytorch, followed by simple model building, similar to what we do in the class."
      ],
      "metadata": {
        "id": "4Qn9bLVIYm20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Torch Tensors**\n",
        "Tensors are the primary data objects in torch, equivalent to numpy array/ndarrays"
      ],
      "metadata": {
        "id": "p2KBzAtlZEdD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh3ETICpKk5b",
        "outputId": "230cac6c-9352-4509-dc74-dbce0b0fad13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Initializing a tensor\n",
        "# you can create tensors in multiple ways like numpy\n",
        "data = torch.tensor([\n",
        "                     [0, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ])\n",
        "zeros = torch.zeros(2, 5)\n",
        "ones = torch.ones(3, 4)\n",
        "rr = torch.arange(1, 10)\n",
        "print(data)\n",
        "print(zeros)\n",
        "print(ones)\n",
        "print(rr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scalar operation in torch\n",
        "print(rr + 2)\n",
        "print(rr * 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL2wT3lnVDLp",
        "outputId": "aa093b61-b644-4665-d56e-b5ccdf1f0afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor operation matrix multiplication in two different ways\n",
        "a = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n",
        "b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)  (3, 4)\n",
        "\n",
        "print(\"The product is\", a.matmul(b))\n",
        "print(\"The other product is\", a @ b) # @"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMNUTkjdV4V2",
        "outputId": "043cf1cf-9aae-4060-a512-e84601a7a2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n",
            "The other product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping tensors\n",
        "rr = torch.arange(1, 16)\n",
        "print(\"The shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)\n",
        "print()\n",
        "rr = rr.view(5, 3)\n",
        "print(\"After reshaping, the shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIQSt_c1WUsu",
        "outputId": "40bff755-59d8-4af7-a4d2-0659ab4374de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape is currently torch.Size([15])\n",
            "The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
            "\n",
            "After reshaping, the shape is currently torch.Size([5, 3])\n",
            "The contents are currently tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12],\n",
            "        [13, 14, 15]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting between numpy and torch\n",
        "import numpy as np\n",
        "\n",
        "# numpy.ndarray --> torch.Tensor:\n",
        "arr = np.array([[1, 0, 5]])\n",
        "data = torch.tensor(arr)\n",
        "print(\"This is a torch.tensor\", data)\n",
        "\n",
        "# torch.Tensor --> numpy.ndarray:\n",
        "new_arr = data.numpy()\n",
        "print(\"This is a np.ndarray\", new_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoQdp2s7WtSn",
        "outputId": "18638d6f-399c-4e4f-f09d-8b4d0f626074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a torch.tensor tensor([[1, 0, 5]])\n",
            "This is a np.ndarray [[1 0 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# operations within a torch tensor\n",
        "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
        "print(\"Data is:\\n\", data)\n",
        "\n",
        "# We can perform operations like *sum* over each row...\n",
        "print(\"Taking the sum over columns:\")\n",
        "print(data.sum(dim=0))\n",
        "\n",
        "# or over each column.\n",
        "print(\"Taking the sum over rows:\")\n",
        "print(data.sum(dim=1))\n",
        "\n",
        "# Other operations are available:\n",
        "print(\"Taking the stdev over rows:\")\n",
        "print(data.std(dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt3-cUI0W23n",
        "outputId": "9d3fce8e-88a6-446e-e2a5-f452d2179d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is:\n",
            " tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19., 20., 21.],\n",
            "        [22., 23., 24., 25., 26., 27., 28.],\n",
            "        [29., 30., 31., 32., 33., 34., 35.]])\n",
            "Taking the sum over columns:\n",
            "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
            "Taking the sum over rows:\n",
            "tensor([ 28.,  77., 126., 175., 224.])\n",
            "Taking the stdev over rows:\n",
            "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.sysinfo import pprint\n",
        "# there are many ways to get element my index\n",
        "print(data[3,1])\n",
        "print(data[1])\n",
        "print(data[:,3])\n",
        "# we can access elements in torch using a mask\n",
        "x_ids = [0,2,4,2]\n",
        "y_ids = [1,3,2,1]\n",
        "print(data[x_ids,y_ids])\n",
        "# use item to get python scalar value\n",
        "print(data[0, 0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl1bgATpXHNL",
        "outputId": "54dbcebe-a65c-4369-c707-da25c7f5fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(23.)\n",
            "tensor([ 8.,  9., 10., 11., 12., 13., 14.])\n",
            "tensor([ 4., 11., 18., 25., 32.])\n",
            "tensor([ 2., 18., 31., 16.])\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Autograd**\n",
        "Pytorch is well-known for its automatic differentiation feature. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."
      ],
      "metadata": {
        "id": "V1W_Pq1EYgib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "print(x)\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0l1APmBXvqU",
        "outputId": "693d785d-0f8b-41fc-86cc-350a892e215f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.], requires_grad=True)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlLPrFGQYj7a",
        "outputId": "a6a426c5-633d-454b-dc8a-1ce6b450aecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYf_5kOMYqq8",
        "outputId": "f767a7b1-9501-4142-b302-9bffea33f324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ],
      "metadata": {
        "id": "GTW92m87Y3As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "2fEbCJlQYyHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Layer**\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ],
      "metadata": {
        "id": "hBgX5a9TZfy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "print(input)\n",
        "# N* H_in -> N*H_out\n",
        "\n",
        "\n",
        "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
        "# dimensional outputs\n",
        "linear = nn.Linear(4, 2)\n",
        "linear_output = linear(input)\n",
        "linear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMwo2nIhZTyo",
        "outputId": "4dcdc65c-e8bc-448a-b306-d11530e65f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2021, -0.0600],\n",
              "         [-0.2021, -0.0600],\n",
              "         [-0.2021, -0.0600]],\n",
              "\n",
              "        [[-0.2021, -0.0600],\n",
              "         [-0.2021, -0.0600],\n",
              "         [-0.2021, -0.0600]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear.parameters()) # Ax + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec0_qF_RZdL7",
        "outputId": "268c880f-b24e-4722-cef9-e198a9d93bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2412,  0.0789, -0.4416,  0.3624],\n",
              "         [ 0.0619,  0.3204, -0.4371, -0.2285]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.0395, 0.2234], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Function Layer**\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
      ],
      "metadata": {
        "id": "A4ocJ5ZVaPYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBk7O3dKZsUk",
        "outputId": "a251df09-0ed1-4e77-9bc5-d11bd7101b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4496, 0.4850],\n",
              "         [0.4496, 0.4850],\n",
              "         [0.4496, 0.4850]],\n",
              "\n",
              "        [[0.4496, 0.4850],\n",
              "         [0.4496, 0.4850],\n",
              "         [0.4496, 0.4850]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that."
      ],
      "metadata": {
        "id": "g-4hUErKaZvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI79AOrLaSXq",
        "outputId": "6b5df218-c39e-42c5-d961-60de074f77a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2666, 0.7752],\n",
              "         [0.2666, 0.7752],\n",
              "         [0.2666, 0.7752]],\n",
              "\n",
              "        [[0.2666, 0.7752],\n",
              "         [0.2666, 0.7752],\n",
              "         [0.2666, 0.7752]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimization**\n",
        "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."
      ],
      "metadata": {
        "id": "-GPeNJata0I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "xyruGnEOaeDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want out model to predict our original data, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIoDA7j0a2hV",
        "outputId": "1d3ddf8f-2b1b-4aad-99fa-0b295071d3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4663,  1.1629,  2.9229,  1.7671,  1.5086],\n",
              "        [ 2.5267,  0.6002, -0.2588,  2.3200,  1.3105],\n",
              "        [ 0.9154,  0.0099,  1.1392,  3.0061,  0.6774],\n",
              "        [ 0.1302,  0.3888, -0.8516,  1.0685,  0.1376],\n",
              "        [ 2.2306, -0.3827,  3.0548, -0.5210,  1.0460],\n",
              "        [ 0.6874,  2.1466,  2.1439,  1.8129, -0.1852],\n",
              "        [ 2.3790, -0.7556,  0.1863,  1.2663,  1.6452],\n",
              "        [ 3.0661,  2.3477, -0.0770,  3.0439,  2.1019],\n",
              "        [ 1.2420, -0.5397,  0.1221, -0.0594, -0.3585],\n",
              "        [ 0.4145,  1.9138,  1.4450, -0.1977,  0.7329]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(5, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3, 5),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Define the optimizer\n",
        "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Calculate how our model is doing now\n",
        "y_pred = model(x)\n",
        "loss_function(y_pred, y).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amg6gBexbBPD",
        "outputId": "39e2f03d-b49e-40f2-86e1-645c5d7343ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8271358609199524"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop."
      ],
      "metadata": {
        "id": "lEUk7lwMcCCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  adam.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xu7J9ICbFCD",
        "outputId": "2728b195-ee0c-4cde-a588-d77d42b67d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: traing loss: 0.8271358609199524\n",
            "Epoch 1: traing loss: 0.6808784604072571\n",
            "Epoch 2: traing loss: 0.6123683452606201\n",
            "Epoch 3: traing loss: 0.5250092148780823\n",
            "Epoch 4: traing loss: 0.4013831317424774\n",
            "Epoch 5: traing loss: 0.27092137932777405\n",
            "Epoch 6: traing loss: 0.1613507717847824\n",
            "Epoch 7: traing loss: 0.0897623747587204\n",
            "Epoch 8: traing loss: 0.051260724663734436\n",
            "Epoch 9: traing loss: 0.030743010342121124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tasks**\n",
        "We will be reconstructing the input like the previous example with a few modifications\n",
        "1. create a torch tensor `y` with size (10,10) with random values in the range (-3,7)\n",
        "2. create `error` with random values from the range (-0.5,0.5) and add this error to y to create our `input`\n",
        "3. create a model with three hidden layers where each layer will consist of a linear unit with a relu non-linear function except for the third/final layer which won't have a non-linear function attached\n",
        "4. The second and third hidden layer will have input dimension of respectively 7 and 11\n",
        "5. run the subsequent code to train the model"
      ],
      "metadata": {
        "id": "YbC5kdSUc1WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo code here\n",
        "torch.manual_seed(13)\n",
        "\n",
        "# 10 x 10 matrix\n",
        "y = torch.randint(-3, 7, (10, 10), dtype=torch.float32)\n",
        "# error should have same size as y\n",
        "error = torch.rand_like(y) * 1.0 - 0.5 # following this formula to create range, y= x*(bâˆ’a)+a\n",
        "# input is a sum of y and error\n",
        "input = y + error\n",
        "# # this is sequential model\n",
        "model2 = nn.Sequential(\n",
        "    nn.Linear(10, 7),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(7, 11),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(11, 10),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# we already preset these\n",
        "adam = optim.Adam(model2.parameters(), lr=1e-1)\n",
        "# we will use mean squared error as loss\n",
        "loss_function = nn.MSELoss()\n",
        "n_epoch = 10\n",
        "for epoch in range(n_epoch):\n",
        "  # reset the grads to make sure it does accumulate values from previous steps\n",
        "  adam.zero_grad()\n",
        "  # getting output using forward pass\n",
        "  y_pred = model2(input)\n",
        "  # calculating loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "  # print\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "  # backward pass: calculate gradient\n",
        "  loss.backward()\n",
        "  # update weights\n",
        "  adam.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sARk7Sb9cDlb",
        "outputId": "9c0a57c9-93af-4a81-c29a-b813c8c5ee8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: traing loss: 9.028261184692383\n",
            "Epoch 1: traing loss: 8.470184326171875\n",
            "Epoch 2: traing loss: 7.989218235015869\n",
            "Epoch 3: traing loss: 7.737677574157715\n",
            "Epoch 4: traing loss: 7.630360126495361\n",
            "Epoch 5: traing loss: 7.662440776824951\n",
            "Epoch 6: traing loss: 7.627350330352783\n",
            "Epoch 7: traing loss: 7.563365936279297\n",
            "Epoch 8: traing loss: 7.502579212188721\n",
            "Epoch 9: traing loss: 7.430051326751709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOwArJLfh0kx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}